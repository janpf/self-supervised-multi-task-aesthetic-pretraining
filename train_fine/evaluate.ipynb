{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path[0] = \"/home/projects/NIAA/\"\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from train_pre.utils import mapping, parameter_range\n",
    "from scipy import stats\n",
    "from pathlib import Path\n",
    "import math\n",
    "import swifter\n",
    "from sklearn import metrics\n",
    "plt.style.reload_library()\n",
    "plt.style.use(['science'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lah analysis/not_uploaded/IA2NIMA/AVA/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_l = []\n",
    "for p in Path(\"analysis/not_uploaded/IA2NIMA/AVA/\").iterdir():\n",
    "    tmp = pd.read_csv(p)\n",
    "    tmp[\"scores\"] = tmp[\"scores\"].swifter.apply(eval)\n",
    "    tmp[\"score\"] = tmp[\"scores\"].swifter.apply(lambda row: sum([row[i] * (i+1) for i in range(len(row))]))\n",
    "    tmp[\"img\"] = tmp[\"img\"].swifter.apply(lambda row: row.split(\".\")[0])\n",
    "    tmp[\"quality\"] = tmp[\"score\"].apply(lambda row: 1 if row > 5 else 0)\n",
    "    tmp[\"quality\"] = tmp[\"quality\"].astype(int)\n",
    "    tmp[\"img\"] = tmp[\"img\"].astype(int)\n",
    "\n",
    "    tmp.drop(columns=[\"scores\"], inplace=True)\n",
    "    tmp = tmp.rename(columns={\"score\":\"score_\" + p.stem.split(\"AVA.\")[1], \"quality\": \"quality_\" + p.stem.split(\"AVA.\")[1]})\n",
    "\n",
    "    tmp = tmp.set_index(\"img\")\n",
    "    df_l.append(tmp)\n",
    "\n",
    "df = df_l[0].join(df_l[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = pd.read_csv(\"analysis/not_uploaded/AVA_gt.txt\", sep=\" \").drop(columns=[\"Unnamed: 0\", \"semanticTagID1\", \"semanticTagID2\", \"challengeID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt[\"votes\"] = gt.apply(lambda row: sum(list(row)[1:]), axis=1)\n",
    "gt[\"gt_score\"] = gt.apply(lambda row: sum([val * (i + 1) for i, val in enumerate(list(row)[1:-1])]), axis=1)\n",
    "gt[\"gt_score\"] = gt.apply(lambda row: row.gt_score / row.votes, axis=1)\n",
    "gt[\"gt_quality\"] = gt[\"gt_score\"].apply(lambda row: 1 if row > 5 else 0)\n",
    "gt[\"img\"] = gt[\"img\"].astype(int)\n",
    "\n",
    "gt = gt[[\"img\",\"gt_score\", \"gt_quality\"]]\n",
    "gt = gt.set_index(\"img\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = gt.join(df).dropna().drop_duplicates()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for score in sorted(df.columns):\n",
    "    if \"gt\" in score or not \"score\" in score:\n",
    "        continue\n",
    "    print()\n",
    "    print(score)\n",
    "    print(\"LCC :\", stats.pearsonr(df[\"gt_score\"], df[score])[0])\n",
    "    print(\"SRCC:\", stats.spearmanr(df[\"gt_score\"], df[score])[0])\n",
    "    print(\"ACC :\", metrics.accuracy_score(df[\"gt_quality\"], df[score.replace(\"score\", \"quality\")]))\n",
    "    print(\"F1  :\", metrics.f1_score(df[\"gt_quality\"], df[score.replace(\"score\", \"quality\")]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wilcoxon\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signific_df = df[[\"gt_score\", \"score_one.change_regress.epoch-77.pth\", \"score_imagenet.epoch-147.pth\"]].rename(columns={\"score_one.change_regress.epoch-77.pth\": \"ours\", \"score_imagenet.epoch-147.pth\":\"imagenet\"})\n",
    "signific_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signific_df[\"our_error\"] = signific_df.apply(lambda row: metrics.mean_absolute_error([row.gt_score], [row.ours]), axis=1)\n",
    "signific_df[\"imagenet_error\"] = signific_df.apply(lambda row: metrics.mean_absolute_error([row.gt_score], [row.imagenet]), axis=1)\n",
    "signific_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signific_df[\"our_error\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signific_df[\"imagenet_error\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wilcoxon(signific_df[\"our_error\"],signific_df[\"imagenet_error\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "wilcoxon(signific_df[\"our_error\"],signific_df[\"imagenet_error\"], alternative=\"less\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}